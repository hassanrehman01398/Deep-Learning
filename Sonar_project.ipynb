{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CjqUHAVpIIVt",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "ca64bb1d-e01f-40dd-ea76-fd22f1be63ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras \n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRSCu-hSIT7U",
    "colab_type": "text"
   },
   "source": [
    "#Next, we can initialize the random number generator to ensure that we always get the same results when executing this code. This will help if we are debugging :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "KKjqiVsGIUwu",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQqC_fXqIayV",
    "colab_type": "text"
   },
   "source": [
    "#Loading the Sonar.csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kvcwEtkqIdmS",
    "colab_type": "code",
    "outputId": "9aec6db5-65a0-44f7-d1d7-44bef309065b",
    "colab": {
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "ok": true,
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "status": 200.0,
       "status_text": ""
      }
     },
     "base_uri": "https://localhost:8080/",
     "height": 157.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "graphviz is already the newest version (2.40.1-2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-bef3fc80-b936-4af9-abbe-b3ccded50a45\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-bef3fc80-b936-4af9-abbe-b3ccded50a45\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sonar.csv to sonar.csv\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "!apt-get install graphviz -y\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "import pandas as pd\n",
    "import io\n",
    "dataframe = pd.read_csv('sonar.csv', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsEBr6bzIqQg",
    "colab_type": "text"
   },
   "source": [
    "#Now we can split the columns into 60 input variables (X) and 1 output variable (Y). We use pandas to load the data because it easily handles strings (the output variable), whereas attempting to load the data directly using NumPy would be more difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qsXW4m6yIroi",
    "colab_type": "code",
    "outputId": "9e3f7916-d78d-4e66-a9f9-dadbbdfd99d2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3247.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0       1       2       3       4       5       6       7       8   \\\n",
      "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
      "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
      "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
      "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
      "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
      "5    0.0286  0.0453  0.0277  0.0174  0.0384  0.0990  0.1201  0.1833  0.2105   \n",
      "6    0.0317  0.0956  0.1321  0.1408  0.1674  0.1710  0.0731  0.1401  0.2083   \n",
      "7    0.0519  0.0548  0.0842  0.0319  0.1158  0.0922  0.1027  0.0613  0.1465   \n",
      "8    0.0223  0.0375  0.0484  0.0475  0.0647  0.0591  0.0753  0.0098  0.0684   \n",
      "9    0.0164  0.0173  0.0347  0.0070  0.0187  0.0671  0.1056  0.0697  0.0962   \n",
      "10   0.0039  0.0063  0.0152  0.0336  0.0310  0.0284  0.0396  0.0272  0.0323   \n",
      "11   0.0123  0.0309  0.0169  0.0313  0.0358  0.0102  0.0182  0.0579  0.1122   \n",
      "12   0.0079  0.0086  0.0055  0.0250  0.0344  0.0546  0.0528  0.0958  0.1009   \n",
      "13   0.0090  0.0062  0.0253  0.0489  0.1197  0.1589  0.1392  0.0987  0.0955   \n",
      "14   0.0124  0.0433  0.0604  0.0449  0.0597  0.0355  0.0531  0.0343  0.1052   \n",
      "15   0.0298  0.0615  0.0650  0.0921  0.1615  0.2294  0.2176  0.2033  0.1459   \n",
      "16   0.0352  0.0116  0.0191  0.0469  0.0737  0.1185  0.1683  0.1541  0.1466   \n",
      "17   0.0192  0.0607  0.0378  0.0774  0.1388  0.0809  0.0568  0.0219  0.1037   \n",
      "18   0.0270  0.0092  0.0145  0.0278  0.0412  0.0757  0.1026  0.1138  0.0794   \n",
      "19   0.0126  0.0149  0.0641  0.1732  0.2565  0.2559  0.2947  0.4110  0.4983   \n",
      "20   0.0473  0.0509  0.0819  0.1252  0.1783  0.3070  0.3008  0.2362  0.3830   \n",
      "21   0.0664  0.0575  0.0842  0.0372  0.0458  0.0771  0.0771  0.1130  0.2353   \n",
      "22   0.0099  0.0484  0.0299  0.0297  0.0652  0.1077  0.2363  0.2385  0.0075   \n",
      "23   0.0115  0.0150  0.0136  0.0076  0.0211  0.1058  0.1023  0.0440  0.0931   \n",
      "24   0.0293  0.0644  0.0390  0.0173  0.0476  0.0816  0.0993  0.0315  0.0736   \n",
      "25   0.0201  0.0026  0.0138  0.0062  0.0133  0.0151  0.0541  0.0210  0.0505   \n",
      "26   0.0151  0.0320  0.0599  0.1050  0.1163  0.1734  0.1679  0.1119  0.0889   \n",
      "27   0.0177  0.0300  0.0288  0.0394  0.0630  0.0526  0.0688  0.0633  0.0624   \n",
      "28   0.0100  0.0275  0.0190  0.0371  0.0416  0.0201  0.0314  0.0651  0.1896   \n",
      "29   0.0189  0.0308  0.0197  0.0622  0.0080  0.0789  0.1440  0.1451  0.1789   \n",
      "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "178  0.0197  0.0394  0.0384  0.0076  0.0251  0.0629  0.0747  0.0578  0.1357   \n",
      "179  0.0394  0.0420  0.0446  0.0551  0.0597  0.1416  0.0956  0.0802  0.1618   \n",
      "180  0.0310  0.0221  0.0433  0.0191  0.0964  0.1827  0.1106  0.1702  0.2804   \n",
      "181  0.0423  0.0321  0.0709  0.0108  0.1070  0.0973  0.0961  0.1323  0.2462   \n",
      "182  0.0095  0.0308  0.0539  0.0411  0.0613  0.1039  0.1016  0.1394  0.2592   \n",
      "183  0.0096  0.0404  0.0682  0.0688  0.0887  0.0932  0.0955  0.2140  0.2546   \n",
      "184  0.0269  0.0383  0.0505  0.0707  0.1313  0.2103  0.2263  0.2524  0.3595   \n",
      "185  0.0340  0.0625  0.0381  0.0257  0.0441  0.1027  0.1287  0.1850  0.2647   \n",
      "186  0.0209  0.0191  0.0411  0.0321  0.0698  0.1579  0.1438  0.1402  0.3048   \n",
      "187  0.0368  0.0279  0.0103  0.0566  0.0759  0.0679  0.0970  0.1473  0.2164   \n",
      "188  0.0089  0.0274  0.0248  0.0237  0.0224  0.0845  0.1488  0.1224  0.1569   \n",
      "189  0.0158  0.0239  0.0150  0.0494  0.0988  0.1425  0.1463  0.1219  0.1697   \n",
      "190  0.0156  0.0210  0.0282  0.0596  0.0462  0.0779  0.1365  0.0780  0.1038   \n",
      "191  0.0315  0.0252  0.0167  0.0479  0.0902  0.1057  0.1024  0.1209  0.1241   \n",
      "192  0.0056  0.0267  0.0221  0.0561  0.0936  0.1146  0.0706  0.0996  0.1673   \n",
      "193  0.0203  0.0121  0.0380  0.0128  0.0537  0.0874  0.1021  0.0852  0.1136   \n",
      "194  0.0392  0.0108  0.0267  0.0257  0.0410  0.0491  0.1053  0.1690  0.2105   \n",
      "195  0.0129  0.0141  0.0309  0.0375  0.0767  0.0787  0.0662  0.1108  0.1777   \n",
      "196  0.0050  0.0017  0.0270  0.0450  0.0958  0.0830  0.0879  0.1220  0.1977   \n",
      "197  0.0366  0.0421  0.0504  0.0250  0.0596  0.0252  0.0958  0.0991  0.1419   \n",
      "198  0.0238  0.0318  0.0422  0.0399  0.0788  0.0766  0.0881  0.1143  0.1594   \n",
      "199  0.0116  0.0744  0.0367  0.0225  0.0076  0.0545  0.1110  0.1069  0.1708   \n",
      "200  0.0131  0.0387  0.0329  0.0078  0.0721  0.1341  0.1626  0.1902  0.2610   \n",
      "201  0.0335  0.0258  0.0398  0.0570  0.0529  0.1091  0.1709  0.1684  0.1865   \n",
      "202  0.0272  0.0378  0.0488  0.0848  0.1127  0.1103  0.1349  0.2337  0.3113   \n",
      "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
      "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
      "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
      "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
      "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
      "\n",
      "         9  ...      51      52      53      54      55      56      57  \\\n",
      "0    0.2111 ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
      "1    0.2872 ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
      "2    0.6194 ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
      "3    0.1264 ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
      "4    0.4459 ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
      "5    0.3039 ...  0.0045  0.0014  0.0038  0.0013  0.0089  0.0057  0.0027   \n",
      "6    0.3513 ...  0.0201  0.0248  0.0131  0.0070  0.0138  0.0092  0.0143   \n",
      "7    0.2838 ...  0.0081  0.0120  0.0045  0.0121  0.0097  0.0085  0.0047   \n",
      "8    0.1487 ...  0.0145  0.0128  0.0145  0.0058  0.0049  0.0065  0.0093   \n",
      "9    0.0251 ...  0.0090  0.0223  0.0179  0.0084  0.0068  0.0032  0.0035   \n",
      "10   0.0452 ...  0.0062  0.0120  0.0052  0.0056  0.0093  0.0042  0.0003   \n",
      "11   0.0835 ...  0.0133  0.0265  0.0224  0.0074  0.0118  0.0026  0.0092   \n",
      "12   0.1240 ...  0.0176  0.0127  0.0088  0.0098  0.0019  0.0059  0.0058   \n",
      "13   0.1895 ...  0.0059  0.0095  0.0194  0.0080  0.0152  0.0158  0.0053   \n",
      "14   0.2120 ...  0.0083  0.0057  0.0174  0.0188  0.0054  0.0114  0.0196   \n",
      "15   0.0852 ...  0.0031  0.0153  0.0071  0.0212  0.0076  0.0152  0.0049   \n",
      "16   0.2912 ...  0.0346  0.0158  0.0154  0.0109  0.0048  0.0095  0.0015   \n",
      "17   0.1186 ...  0.0331  0.0131  0.0120  0.0108  0.0024  0.0045  0.0037   \n",
      "18   0.1520 ...  0.0084  0.0010  0.0018  0.0068  0.0039  0.0120  0.0132   \n",
      "19   0.5920 ...  0.0092  0.0035  0.0098  0.0121  0.0006  0.0181  0.0094   \n",
      "20   0.3759 ...  0.0193  0.0118  0.0064  0.0042  0.0054  0.0049  0.0082   \n",
      "21   0.1838 ...  0.0141  0.0190  0.0043  0.0036  0.0026  0.0024  0.0162   \n",
      "22   0.1882 ...  0.0173  0.0149  0.0115  0.0202  0.0139  0.0029  0.0160   \n",
      "23   0.0734 ...  0.0091  0.0016  0.0084  0.0064  0.0026  0.0029  0.0037   \n",
      "24   0.0860 ...  0.0035  0.0052  0.0083  0.0078  0.0075  0.0105  0.0160   \n",
      "25   0.1097 ...  0.0108  0.0070  0.0063  0.0030  0.0011  0.0007  0.0024   \n",
      "26   0.1205 ...  0.0061  0.0015  0.0084  0.0128  0.0054  0.0011  0.0019   \n",
      "27   0.0613 ...  0.0102  0.0122  0.0044  0.0075  0.0124  0.0099  0.0057   \n",
      "28   0.2668 ...  0.0088  0.0104  0.0036  0.0088  0.0047  0.0117  0.0020   \n",
      "29   0.2522 ...  0.0038  0.0096  0.0142  0.0190  0.0140  0.0099  0.0092   \n",
      "..      ... ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "178  0.1695 ...  0.0134  0.0097  0.0042  0.0058  0.0072  0.0041  0.0045   \n",
      "179  0.2558 ...  0.0146  0.0040  0.0114  0.0032  0.0062  0.0101  0.0068   \n",
      "180  0.4432 ...  0.0204  0.0059  0.0053  0.0079  0.0037  0.0015  0.0056   \n",
      "181  0.2696 ...  0.0176  0.0035  0.0093  0.0121  0.0075  0.0056  0.0021   \n",
      "182  0.3745 ...  0.0181  0.0019  0.0102  0.0133  0.0040  0.0042  0.0030   \n",
      "183  0.2952 ...  0.0237  0.0078  0.0144  0.0170  0.0012  0.0109  0.0036   \n",
      "184  0.5915 ...  0.0167  0.0199  0.0145  0.0081  0.0045  0.0043  0.0027   \n",
      "185  0.4117 ...  0.0141  0.0019  0.0067  0.0099  0.0042  0.0057  0.0051   \n",
      "186  0.3914 ...  0.0078  0.0201  0.0104  0.0039  0.0031  0.0062  0.0087   \n",
      "187  0.2544 ...  0.0105  0.0024  0.0018  0.0057  0.0092  0.0009  0.0086   \n",
      "188  0.2119 ...  0.0096  0.0103  0.0093  0.0025  0.0044  0.0021  0.0069   \n",
      "189  0.1923 ...  0.0121  0.0108  0.0057  0.0028  0.0079  0.0034  0.0046   \n",
      "190  0.1567 ...  0.0150  0.0060  0.0082  0.0091  0.0038  0.0056  0.0056   \n",
      "191  0.1533 ...  0.0108  0.0062  0.0044  0.0072  0.0007  0.0054  0.0035   \n",
      "192  0.1859 ...  0.0072  0.0055  0.0074  0.0068  0.0084  0.0037  0.0024   \n",
      "193  0.1747 ...  0.0134  0.0094  0.0047  0.0045  0.0042  0.0028  0.0036   \n",
      "194  0.2471 ...  0.0083  0.0080  0.0026  0.0079  0.0042  0.0071  0.0044   \n",
      "195  0.2245 ...  0.0124  0.0093  0.0072  0.0019  0.0027  0.0054  0.0017   \n",
      "196  0.2282 ...  0.0165  0.0056  0.0010  0.0027  0.0062  0.0024  0.0063   \n",
      "197  0.1847 ...  0.0132  0.0027  0.0022  0.0059  0.0016  0.0025  0.0017   \n",
      "198  0.2048 ...  0.0096  0.0071  0.0084  0.0038  0.0026  0.0028  0.0013   \n",
      "199  0.2271 ...  0.0141  0.0103  0.0100  0.0034  0.0026  0.0037  0.0044   \n",
      "200  0.3193 ...  0.0150  0.0076  0.0032  0.0037  0.0071  0.0040  0.0009   \n",
      "201  0.2660 ...  0.0120  0.0039  0.0053  0.0062  0.0046  0.0045  0.0022   \n",
      "202  0.3997 ...  0.0091  0.0045  0.0043  0.0043  0.0098  0.0054  0.0051   \n",
      "203  0.2684 ...  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065  0.0115   \n",
      "204  0.2154 ...  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034  0.0032   \n",
      "205  0.2529 ...  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140  0.0138   \n",
      "206  0.2354 ...  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034  0.0079   \n",
      "207  0.2354 ...  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040  0.0036   \n",
      "\n",
      "         58      59  60  \n",
      "0    0.0090  0.0032   R  \n",
      "1    0.0052  0.0044   R  \n",
      "2    0.0095  0.0078   R  \n",
      "3    0.0040  0.0117   R  \n",
      "4    0.0107  0.0094   R  \n",
      "5    0.0051  0.0062   R  \n",
      "6    0.0036  0.0103   R  \n",
      "7    0.0048  0.0053   R  \n",
      "8    0.0059  0.0022   R  \n",
      "9    0.0056  0.0040   R  \n",
      "10   0.0053  0.0036   R  \n",
      "11   0.0009  0.0044   R  \n",
      "12   0.0059  0.0032   R  \n",
      "13   0.0189  0.0102   R  \n",
      "14   0.0147  0.0062   R  \n",
      "15   0.0200  0.0073   R  \n",
      "16   0.0073  0.0067   R  \n",
      "17   0.0112  0.0075   R  \n",
      "18   0.0070  0.0088   R  \n",
      "19   0.0116  0.0063   R  \n",
      "20   0.0028  0.0027   R  \n",
      "21   0.0109  0.0079   R  \n",
      "22   0.0106  0.0134   R  \n",
      "23   0.0070  0.0041   R  \n",
      "24   0.0095  0.0011   R  \n",
      "25   0.0057  0.0044   R  \n",
      "26   0.0023  0.0062   R  \n",
      "27   0.0032  0.0019   R  \n",
      "28   0.0091  0.0058   R  \n",
      "29   0.0052  0.0075   R  \n",
      "..      ...     ...  ..  \n",
      "178  0.0047  0.0054   M  \n",
      "179  0.0053  0.0087   M  \n",
      "180  0.0067  0.0054   M  \n",
      "181  0.0043  0.0017   M  \n",
      "182  0.0031  0.0033   M  \n",
      "183  0.0043  0.0018   M  \n",
      "184  0.0055  0.0057   M  \n",
      "185  0.0033  0.0058   M  \n",
      "186  0.0070  0.0042   M  \n",
      "187  0.0110  0.0052   M  \n",
      "188  0.0060  0.0018   M  \n",
      "189  0.0022  0.0021   M  \n",
      "190  0.0048  0.0024   M  \n",
      "191  0.0001  0.0055   M  \n",
      "192  0.0034  0.0007   M  \n",
      "193  0.0013  0.0016   M  \n",
      "194  0.0022  0.0014   M  \n",
      "195  0.0024  0.0029   M  \n",
      "196  0.0017  0.0028   M  \n",
      "197  0.0027  0.0027   M  \n",
      "198  0.0035  0.0060   M  \n",
      "199  0.0057  0.0035   M  \n",
      "200  0.0015  0.0085   M  \n",
      "201  0.0005  0.0031   M  \n",
      "202  0.0065  0.0103   M  \n",
      "203  0.0193  0.0157   M  \n",
      "204  0.0062  0.0067   M  \n",
      "205  0.0077  0.0031   M  \n",
      "206  0.0036  0.0048   M  \n",
      "207  0.0061  0.0115   M  \n",
      "\n",
      "[208 rows x 61 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8cO06sR2JJjk",
    "colab_type": "code",
    "outputId": "d4114954-9dc6-472e-8bf9-d202b6e9071c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02 0.0371 0.0428 ... 0.009 0.0032 'R']\n",
      " [0.0453 0.0523 0.0843 ... 0.0052 0.0044 'R']\n",
      " [0.0262 0.0582 0.1099 ... 0.0095 0.0078 'R']\n",
      " ...\n",
      " [0.0522 0.0437 0.018 ... 0.0077 0.0031 'M']\n",
      " [0.0303 0.0353 0.049 ... 0.0036 0.0048 'M']\n",
      " [0.026 0.0363 0.0136 ... 0.0061 0.0115 'M']]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataframe.values\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cizD9g7mJPdp",
    "colab_type": "code",
    "outputId": "7895a63a-e367-4d0f-833c-8a6a374f9712",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02   0.0371 0.0428 ... 0.0084 0.009  0.0032]\n",
      " [0.0453 0.0523 0.0843 ... 0.0049 0.0052 0.0044]\n",
      " [0.0262 0.0582 0.1099 ... 0.0164 0.0095 0.0078]\n",
      " ...\n",
      " [0.0522 0.0437 0.018  ... 0.0138 0.0077 0.0031]\n",
      " [0.0303 0.0353 0.049  ... 0.0079 0.0036 0.0048]\n",
      " [0.026  0.0363 0.0136 ... 0.0036 0.0061 0.0115]]\n",
      "['R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M']\n"
     ]
    }
   ],
   "source": [
    "x = dataset[:,0:60].astype(float)    #splitting\n",
    "y = dataset[:,60]\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlRUIBHoJaql",
    "colab_type": "text"
   },
   "source": [
    "#The output variable is string values. We must convert them into integer values 0 and 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tOs0L1e6JcC1",
    "colab_type": "code",
    "outputId": "b2fd4383-24aa-4172-b310-a67d25ccee9c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "encoded_Y = le.fit_transform(y)\n",
    "print(encoded_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmiavMDcJigG",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "\n",
    "#* Our model will have a single fully connected hidden layer with the same number of neurons as input variables. This is a good default starting point when creating neural networks.\n",
    "\n",
    "#* The weights are initialized using a small Gaussian random number. The Rectifier activation function is used. The output layer contains a single neuron in order to make predictions. It uses the sigmoid activation function in order to produce a probability output in the range of 0 to 1 that can easily and automatically be converted to crisp class values. \n",
    "\n",
    "#*  Finally, we are using the logarithmic loss function (binary_crossentropy) during training, the preferred loss function for binary classification problems. The model also uses the efficient Adam optimization algorithm for gradient descent and accuracy metrics will be collected when the model is trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "wGsJn3zCJjcW",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from keras import initializers\n",
    "Gaussian = initializers.random_normal()\n",
    "\n",
    "# baseline model\n",
    "\n",
    "def create_baseline():\n",
    "  \n",
    "  #creating model\n",
    "  \n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(60, kernel_initializer = Gaussian , activation = 'relu',  input_dim=60))\n",
    "  model.add(layers.Dense(1, activation = 'sigmoid' ))\n",
    "\t\n",
    "  #Compiling the model\n",
    "  \n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\t\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fp_vIf73Jsqk",
    "colab_type": "text"
   },
   "source": [
    "#* Now it is time to evaluate this model using stratified cross validation in the scikit-learn framework.\n",
    "\n",
    "#* We pass the number of training epochs to the KerasClassifier, again using reasonable default values. Verbose output is also turned off given that the model will be created 10 times for the 10-fold cross validation being performed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "hSVORsnzJzSH",
    "colab_type": "code",
    "outputId": "dc270c9e-9ceb-4b9d-dfd6-1f23e01a974b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 83.71% (5.75%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with standardized dataset\n",
    "\n",
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x ,encoded_Y, cv=kfold)\n",
    "print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZJFWt4sJ5w0",
    "colab_type": "text"
   },
   "source": [
    "#Re-Run The Baseline Model With Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "VoHdgGbNJ-Yf",
    "colab_type": "code",
    "outputId": "d3b6d17e-5284-4b5a-e718-3a67b1c1b72b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 85.06% (6.70%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate baseline model with standardized dataset\n",
    "\n",
    "np.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, encoded_Y, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuMGSn0QKF1a",
    "colab_type": "text"
   },
   "source": [
    "# #Tuning Layers and Number of Neurons in The Model\n",
    "\n",
    "There are many things to tune on a neural network, such as the weight initialization, activation functions, optimization procedure and so on.\n",
    "\n",
    "One aspect that may have an outsized effect is the structure of the network itself called the network topology. In this section, we take a look at two experiments on the structure of the network: making it smaller and making it larger.\n",
    "[link text](https://)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR83vz25KK1m",
    "colab_type": "text"
   },
   "source": [
    "#1) Evaluate a Smaller Network\n",
    "\n",
    "In this experiment, we take our baseline model with 60 neurons in the hidden layer and reduce it by half to 30. This will put pressure on the network during training to pick out the most important structure in the input data to model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "rW5H_bz_NPML",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# smaller model\n",
    "\n",
    "def create_smaller():\n",
    "  \n",
    "  #Create model\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(30,  activation = 'relu',  input_dim=60))\n",
    "  model.add(layers.Dense(1, activation = 'sigmoid' ))\n",
    "  \n",
    "  # Compile model  \n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  \n",
    "  \n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "P545_beUNfLV",
    "colab_type": "code",
    "outputId": "8a56b8f6-1e1a-4709-d675-138f23b6a6f3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smaller: 83.09% (6.41%)\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=100, batch_size=6, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, encoded_Y, cv=kfold)\n",
    "print(\"Smaller: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFc7qFn0NlrG",
    "colab_type": "text"
   },
   "source": [
    "Running the above code we can see that we have a very slight boost in the mean estimated accuracy and an important reduction in the standard deviation (average spread) of the accuracy scores for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3kd56VCNnL9",
    "colab_type": "text"
   },
   "source": [
    "#2) Evaluate a Larger Network\n",
    "\n",
    "We can evaluate whether adding more layers to the network improves the performance easily by making another small tweak to the function used to create our model. Here, we add one new layer (one line) to the network that introduces another hidden layer with 30 neurons after the first hidden layer.\n",
    "\n",
    "Our network now has the topology:\n",
    "\n",
    "60 inputs -> [60 -> 30] -> 1 output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "EPiuv1AyN9gc",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# larger model\n",
    "\n",
    "def create_larger():\n",
    "  \n",
    "\t# create model\n",
    "  \n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(60 , activation = 'relu' , input_dim = 60 ))\n",
    "  model.add(layers.Dense(30 , activation = 'relu'))\n",
    "  model.add(layers.Dense(1 , activation = 'sigmoid'))\n",
    "\t\n",
    "\t# Compile model\n",
    "  \n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\t\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "wrvhf119OAkf",
    "colab_type": "code",
    "outputId": "3ae725ae-6b2b-4bc4-9afa-a2cfde49e812",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 85.09% (7.25%)\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_larger, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, encoded_Y, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYmpNZZTONoV",
    "colab_type": "text"
   },
   "source": [
    "Running the above code produces the results below. We can see that we do not get a lift in the model performance. This may be statistical noise or a sign that further training is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fBdwM2EOOMR",
    "colab_type": "text"
   },
   "source": [
    "# Rewriting the code using the Keras Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "sAFGrDu9RDfw",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "def func_API():\n",
    "    \n",
    "  # Build model\n",
    "\n",
    "  input = keras.Input(shape =(60,))\n",
    "  x = layers.Dense(60 , activation = 'relu')(input)\n",
    "  output = layers.Dense(1 , activation = 'sigmoid')(x)\n",
    "  model = keras.Model(input,output)\n",
    "\n",
    "  # Compiled Model\n",
    "\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam',   metrics=['accuracy'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "qWxL7ERpTsMD",
    "colab_type": "code",
    "outputId": "6c107e08-a1ef-40ca-add1-29845f03508b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functional API method : 84.11% (8.04%)\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=func_API, epochs=100, batch_size=6, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, encoded_Y, cv=kfold)\n",
    "print(\"Functional API method : %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bLb5O3rWoXX",
    "colab_type": "text"
   },
   "source": [
    "#Rewriting the code by doing Model Subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "au9Wl3t4Wpi0",
    "colab_type": "code",
    "outputId": "55ee0f44-c64e-4890-88ff-0543c3871870",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model SubClass API method : 55.35% (8.99%)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "from keras import layers\n",
    "\n",
    "def SubClass_API():\n",
    "\n",
    "  class MyModel(tf.keras.Model):\n",
    "  \n",
    "    def __init__(self):\n",
    "      super(MyModel, self).__init__()\n",
    "    \n",
    "      self.dense1 = layers.Dense(60 , activation = 'relu')\n",
    "      self.dense2 = layers.Dense(30 , activation = 'relu')\n",
    "      self.dense3 = layers.Dense(1 , activation = 'sigmoid')\n",
    "  \n",
    "    def call(self , inputs):\n",
    "  \n",
    "      x = self.dense1(inputs)\n",
    "      x = self.dense2(x)\n",
    "  \n",
    "      return self.dense3(x) \n",
    "\n",
    "  model = MyModel()\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam',   metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=SubClass_API, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, encoded_Y, cv=kfold)\n",
    "print(\"Model SubClass API method : %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZHsy4Y-pioN",
    "colab_type": "text"
   },
   "source": [
    "#Rewriting the code without using scikit-learn\n",
    "Once you have written the model in all three API style you are required to do k-fold cross validation without using scikit-learn library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ijvIgUqQuumm",
    "colab_type": "code",
    "outputId": "3a9b4126-ca05-4d49-b81f-8b63b50886a1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "processing fold # 1\n",
      "processing fold # 2\n",
      "processing fold # 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "k=4\n",
    "num_val_samples = len(x) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  \n",
    "  #Prepares the validation data : data from partition #k\n",
    "  \n",
    "  val_data = x[i * num_val_samples: (i + 1) * num_val_samples]     \n",
    "  val_targets = encoded_Y[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  \n",
    " \n",
    "  #Prepares the training data : data from all other partitions\n",
    "  \n",
    "  partial_train_data = np.concatenate([x[:i * num_val_samples],x[(i + 1) * num_val_samples:]],axis=0)\n",
    "  partial_train_targets = np.concatenate([encoded_Y[:i * num_val_samples],encoded_Y[(i + 1) * num_val_samples:]],axis=0)\n",
    "  \n",
    " #Builds the Keras model (already compiled)\n",
    "\n",
    "  model = create_baseline()\n",
    "  \n",
    "  #Trains the model(in silent mode,verbose = 0)\n",
    "  \n",
    "  model.fit(partial_train_data, partial_train_targets, epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  \n",
    "  #Evaluates the model on the validation data\n",
    "\n",
    "  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "  all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KjQXJ3nawAve",
    "colab_type": "code",
    "outputId": "669fec83-9867-48d2-8c7f-d280874bdc40",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-fold cross validation : 36.54%  (18.69%)\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(all_scores)*100\n",
    "SD = np.std(all_scores)*100\n",
    "print(\"K-fold cross validation : %.2f%%  (%.2f%%)\" %(mean , SD))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sonar project.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
